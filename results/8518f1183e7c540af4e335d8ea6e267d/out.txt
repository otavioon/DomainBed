Environment:
	Python: 3.10.12
	PyTorch: 2.7.1+cu126
	Torchvision: 0.22.1+cu126
	CUDA: 12.6
	CUDNN: 90501
	NumPy: 2.2.6
	PIL: 11.2.1
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 1
	output_dir: results/8518f1183e7c540af4e335d8ea6e267d
	save_model_every_checkpoint: False
	seed: 1378709529
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 39
	class_balanced: False
	data_augmentation: True
	dinov2: False
	freeze_bn: False
	lars: False
	linear_steps: 500
	lr: 2.7028930742148706e-05
	nonlinear_classifier: False
	resnet18: False
	resnet50_augmix: True
	resnet_dropout: 0.5
	vit: False
	vit_attn_tune: False
	vit_dropout: 0.1
	weight_decay: 0.00044832883881609976
/workspaces/DomainGeneralization/DomainBed/domainbed-venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/workspaces/DomainGeneralization/DomainBed/domainbed-venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        step          step_time    
0.1183648566  0.1393643032  0.1551172708  0.1816239316  0.0898203593  0.0928143713  0.0941475827  0.1031847134  0.0000000000  2.0027961731  9.6385622025  0             1.5683789253 
0.9884075656  0.9559902200  0.9872068230  0.9615384615  0.9827844311  0.9820359281  0.9793256997  0.9528662420  8.7574850299  0.4394951784  9.8230137825  300           0.3168088706 
0.9975594875  0.9535452323  0.9989339019  0.9594017094  0.9895209581  0.9910179641  0.9879134860  0.9401273885  17.514970059  0.0501688257  9.8230137825  600           0.3177287149 
0.9981696156  0.9633251834  0.9984008529  0.9679487179  0.9850299401  0.9880239521  0.9955470738  0.9515923567  26.272455089  0.0248636619  9.8230137825  900           0.3194651167 
0.9975594875  0.9657701711  0.9984008529  0.9679487179  0.9820359281  0.9880239521  0.9939567430  0.9554140127  35.029940119  0.0179978284  9.8230137825  1200          0.3191631023 
0.9993898719  0.9682151589  0.9994669510  0.9764957265  0.9850299401  0.9790419162  0.9980916031  0.9617834395  43.787425149  0.0125850408  9.8230137825  1500          0.3185339967 
0.9987797437  0.9535452323  0.9989339019  0.9700854701  0.9812874251  0.9880239521  0.9984096692  0.9617834395  52.544910179  0.0110565266  9.8230137825  1800          0.3195235912 
0.9993898719  0.9608801956  1.0000000000  0.9743589744  0.9865269461  0.9790419162  0.9987277354  0.9477707006  61.302395209  0.0082450690  9.8230137825  2100          0.3192262085 
0.9981696156  0.9511002445  0.9994669510  0.9615384615  0.9835329341  0.9730538922  0.9965012723  0.9490445860  70.059880239  0.0093991705  9.8230137825  2400          0.3202751541 
1.0000000000  0.9559902200  1.0000000000  0.9786324786  0.9827844311  0.9730538922  0.9996819338  0.9707006369  78.817365269  0.0100848621  9.8230137825  2700          0.3191907342 
1.0000000000  0.9633251834  0.9994669510  0.9764957265  0.9820359281  0.9760479042  0.9974554707  0.9681528662  87.574850299  0.0070592113  9.8230137825  3000          0.3201815653 
0.9993898719  0.9486552567  1.0000000000  0.9743589744  0.9760479042  0.9760479042  0.9977735369  0.9668789809  96.332335329  0.0072279855  9.8230137825  3300          0.3179562362 
0.9987797437  0.9633251834  1.0000000000  0.9807692308  0.9835329341  0.9760479042  0.9980916031  0.9554140127  105.08982035  0.0083419079  9.8230137825  3600          0.3183614842 
0.9987797437  0.9462102689  0.9994669510  0.9807692308  0.9775449102  0.9850299401  0.9965012723  0.9643312102  113.84730538  0.0078813501  9.8230137825  3900          0.3172125435 
0.9981696156  0.9682151589  0.9989339019  0.9615384615  0.9835329341  0.9760479042  0.9993638677  0.9617834395  122.60479041  0.0054572274  9.8230137825  4200          0.3163785330 
0.9981696156  0.9584352078  0.9989339019  0.9700854701  0.9842814371  0.9730538922  0.9990458015  0.9643312102  131.36227544  0.0053897555  9.8230137825  4500          0.3153519464 
1.0000000000  0.9486552567  0.9994669510  0.9615384615  0.9775449102  0.9640718563  0.9980916031  0.9528662420  140.11976047  0.0067150764  9.8230137825  4800          0.3137135156 
0.9993898719  0.9657701711  0.9994669510  0.9743589744  0.9820359281  0.9670658683  0.9974554707  0.9630573248  145.95808383  0.0038648641  9.8230137825  5000          0.3138022232 
